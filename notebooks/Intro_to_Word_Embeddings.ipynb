{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-payroll",
   "metadata": {},
   "source": [
    "# Intro to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cd420",
   "metadata": {},
   "source": [
    "In this notebook we learn about word2vec as a new technique to convert words into (dense) vectors, and re-build the text classification pipeline we had previously built with sparse vectors.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, so some functions are re-used from previous lectures to provide a self-sufficient script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global imports\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b028b",
   "metadata": {},
   "source": [
    "## Bonus: perceptron as the simplest NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ce2e6",
   "metadata": {},
   "source": [
    "Taking the code (with some minor tweaks) from the fantastic Perceptron class here (https://www.thomascountz.com/2018/04/05/19-line-line-by-line-python-perceptron), this is a simple and transparent implementation of a Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs):\n",
    "        # initialize the w + bias array\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "        \n",
    "        return\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        \n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=100, learning_rate=0.01):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                # update the weights\n",
    "                self.weights[1:] += learning_rate * (label - prediction) * inputs\n",
    "                # update the bias\n",
    "                self.weights[0] += learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd47ca",
   "metadata": {},
   "source": [
    "To understand how the forward pass works, let's do some quick calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 1]\n",
    "weights = [0, 0]\n",
    "bias = 1 \n",
    "inputs_dot_weights = np.dot(np.array(inputs), np.array(weights))\n",
    "_sum = inputs_dot_weights + bias\n",
    "\n",
    "print(inputs_dot_weights, _sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f577e6",
   "metadata": {},
   "source": [
    "We can use the class to learn some real-world function of interest, for example the AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's learn the AND operator\n",
    "training_inputs = []\n",
    "training_inputs.append(np.array([1, 1]))\n",
    "training_inputs.append(np.array([1, 0]))\n",
    "training_inputs.append(np.array([0, 1]))\n",
    "training_inputs.append(np.array([0, 0]))\n",
    "# 1 only when both inputs are 1\n",
    "labels = np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate class and train\n",
    "perceptron = Perceptron(2)\n",
    "# weights before training\n",
    "print(perceptron.weights)\n",
    "perceptron.train(training_inputs, labels)\n",
    "# weights after training\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5252f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perceptron.predict(np.array([1, 1])))\n",
    "print(perceptron.predict(np.array([0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab0544",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff40c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [[pre_process_sentence(_['sentence']), _['label']] for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_dataset = get_finance_sentences()\n",
    "# print out the first items in the dataset, to check the format\n",
    "finance_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences without label for vectorizer part\n",
    "finance_dataset_sentences = [_[0] for _ in finance_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-season",
   "metadata": {},
   "source": [
    "## From words to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-seller",
   "metadata": {},
   "source": [
    "As you may recall, we introduced some \"vectorizing\" procedures for text before, e.g. TfidfVectorizer. As you may recall, these vectors are very long and sparse - we quikcly re-create some here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "docs = finance_dataset_sentences[:2]\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(docs)\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data=tfidf_wm.toarray(),\n",
    "                            index=['Doc{}'.format(_) for _ in range(len(docs))], \n",
    "                            columns=tfidf_tokens)\n",
    "print(\"TD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d602a",
   "metadata": {},
   "source": [
    "Let us know use word2vec to get vectors for words first, and document after. We will use a fantastic Python library, gensim: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(\n",
    "    sentences: list,\n",
    "    min_count: int = 2,\n",
    "    vector_size: int = 48,\n",
    "    window: int = 2,\n",
    "    epochs: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Sentences is a list of lists, where each list is composed by tokens in a sentence: e.g.\n",
    "    \n",
    "    [\n",
    "        ['the', 'cat', 'is', 'on' ...],\n",
    "        ['i', 'live', 'in', 'nyc', ...],\n",
    "        ....\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    model =  gensim.models.Word2Vec(sentences=sentences,\n",
    "                                    min_count=min_count,\n",
    "                                    vector_size=vector_size,\n",
    "                                    window=window,\n",
    "                                    epochs=epochs)\n",
    "    \n",
    "    # this is how many words we will have in the space\n",
    "    print(\"# words in the space: {}\".format(len(model.wv.index_to_key)))\n",
    "\n",
    "    # we return the space in a format that will allow us to do nice things afterwards ;-)    \n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use nltk tokenizer to break up sentences and build a word2vec model\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(finance_dataset_sentences[0], '\\n\\n', word_tokenize(finance_dataset_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(_) for _ in finance_dataset_sentences]\n",
    "# debug \n",
    "tokenized_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a counter to get a sense of the lexicon\n",
    "word_counter = Counter([item for sent in tokenized_sentences for item in sent])\n",
    "word_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ca10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = train_word2vec_model(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca8664",
   "metadata": {},
   "source": [
    "Now that we have a vector space, let's find words similar to a given term..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), w2v_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: what happens as the window grows bigger? What is your prediction?\n",
    "\n",
    "# w2v_model = train_word2vec_model(tokenized_sentences, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467f3a2",
   "metadata": {},
   "source": [
    "To get a sense of what the vectors look like, we print them out in 2D using TSNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_by_category_with_lookup(title, \n",
    "                                         words, \n",
    "                                         word_to_target_cat,\n",
    "                                         results):\n",
    "    \"\"\"\n",
    "    Just a plotting routine\n",
    "    \"\"\"\n",
    "    \n",
    "    groups = {}\n",
    "    for word, target_cat in word_to_target_cat.items():\n",
    "        if word not in words:\n",
    "            continue\n",
    "\n",
    "        word_idx = words.index(word)\n",
    "        x = results[word_idx][0]\n",
    "        y = results[word_idx][1]\n",
    "        if target_cat in groups:\n",
    "            groups[target_cat]['x'].append(x)\n",
    "            groups[target_cat]['y'].append(y)\n",
    "        else:\n",
    "            groups[target_cat] = {\n",
    "                'x': [x], 'y': [y]\n",
    "                }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for group, data in groups.items():\n",
    "        ax.scatter(data['x'], data['y'], \n",
    "                   alpha=0.1 if group == 0 else 0.8, \n",
    "                   edgecolors='none', \n",
    "                   s=25, \n",
    "                   marker='o',\n",
    "                   label=group)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_analysis(embeddings, perplexity=25, n_iter=500):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=n_iter)\n",
    "    return tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to known categories of interest\n",
    "\n",
    "# 0 is the generic category\n",
    "words = w2v_model.index_to_key\n",
    "print(len(words))\n",
    "words_to_category = {w: 0 for w in words}\n",
    "# manually pick some words to display\n",
    "for w in ['company', 'profit', 'investment', 'loss', 'margin', 'group']:\n",
    "    words_to_category[w] = 1\n",
    "for w in ['with', 'of', 'from', 'by', 'as']:\n",
    "    words_to_category[w] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [w2v_model[w] for w in words]\n",
    "tsne_results = tsne_analysis(embeddings)\n",
    "assert len(tsne_results) == len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Finance word2vec', words, words_to_category, tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b393ff0",
   "metadata": {},
   "source": [
    "_Why the quality is not ideal?_\n",
    "\n",
    "Our dataset is very small, and word2vec works much better when large corpora are used. However, a pretty cool things of language is that is everywhere: the word \"company\" is very important in the financial sector, but of course also Wikipedia talks a lot about companies... can we make use of all the text out there?\n",
    "\n",
    "The answer is YES: in particular, a pattern that is common to many NLP (but also vision-related) tasks is to initialize a model with PRE-TRAINED embeddings, obtained previously with training on large corpora. We could either re-use them or \"fine-tune\" them: in either case, we will, so to speak, be able to harness the power of Wikipedia even in a corpus very small such as ours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d488aa",
   "metadata": {},
   "source": [
    "### Bonus: using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-strengthening",
   "metadata": {},
   "source": [
    "Here we use Gensim-data to recover dense vectors for words in our vocabulary, as pre-trained on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia\n",
    "pre_trained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# test it out\n",
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), pre_trained_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[w for w in w2v_model.index_to_key if w in pre_trained_model]\n",
    "print(len(words))\n",
    "pre_trained_vectors = [pre_trained_model[w] for w in words]\n",
    "pre_trained_tsne_results = tsne_analysis(pre_trained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6554dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Finance pre-trained word2vec', \n",
    "                                     words, \n",
    "                                     words_to_category, \n",
    "                                     pre_trained_tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9364273",
   "metadata": {},
   "source": [
    "## Application: Text Classification Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efe9b6",
   "metadata": {},
   "source": [
    "As you may recall, one text is in a vectorized form, the downstream pipeline we learned through scikit can be applied in the same way to language dataset. For convenience, we report again a standard classifier for financial news built with TF-IDF transformation first, and then use word2vec to the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "finance_dataset_text = [_[0] for _ in finance_dataset]\n",
    "finance_dataset_label = [_[1] for _ in finance_dataset]\n",
    "all_labels = set(finance_dataset_label)\n",
    "print(\"All labels are: {}\".format(all_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(finance_dataset_text, \n",
    "                                                    finance_dataset_label, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "final_tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "final_tfidf_train = final_tfidfvectorizer.fit_transform(X_train)\n",
    "print(final_tfidf_train.shape)\n",
    "X_test_transformed = final_tfidfvectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(final_tfidf_train, y_train)\n",
    "predicted = model.predict(X_test_transformed)\n",
    "predicted_prob = model.predict_proba(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def calculate_confusion_matrix_and_report(y_predicted, y_golden, with_plot=True):\n",
    "    # calculate confusion matrix: \n",
    "    cm = confusion_matrix(y_golden, y_predicted)\n",
    "    # build a readable report;\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_golden, y_predicted))\n",
    "    # plot the matrix\n",
    "    if with_plot:\n",
    "        plot_confusion_matrix(cm)\n",
    "                                          \n",
    "    return\n",
    "                                          \n",
    "def plot_confusion_matrix(c_matrix):\n",
    "    plt.imshow(c_matrix, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9740d",
   "metadata": {},
   "source": [
    "Let us know transform sentences using word2vec - we go through each of the sentence, remove stop words and take the average of the vector if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ada8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug some vars to make sure all is in order\n",
    "print(w2v_model.most_similar(\"company\"))\n",
    "print(X_train[0],y_train[0])\n",
    "print(X_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, stop_words):\n",
    "    return [w for w in word_tokenize(sentence) if w not in stop_words]\n",
    "\n",
    "\n",
    "def sentence_to_embedding(sentence, model, stop_words, dims=48):\n",
    "    tokenized_sentence = tokenize(sentence, stop_words)\n",
    "    if not tokenized_sentence:\n",
    "        print(\"\\n!!!ATTENTION!!! Empty sentence: {}\".format(sentence))\n",
    "        return np.zeros(dims)\n",
    "    mean_array = np.mean([model[w] for w in tokenized_sentence if w in model] or [np.zeros(dims)], axis=0)\n",
    "    assert len(mean_array) == dims\n",
    "    \n",
    "    return np.array(mean_array)\n",
    "\n",
    "# debug\n",
    "_test = 'company profits were soaring last year'\n",
    "print(tokenize(_test, stop_words))\n",
    "print(sentence_to_embedding(_test, w2v_model, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: instead of taking the average, can we weight \"more\" embeddings which are more important?\n",
    "# e.g. can we use tf-idf as a weighting scheme to aggregate word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a21c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_X_train = np.array([sentence_to_embedding(_, w2v_model, stop_words) for _ in X_train])\n",
    "w2vec_X_test = np.array([sentence_to_embedding(_, w2v_model, stop_words) for _ in X_test])\n",
    "print(len(w2vec_X_train))\n",
    "w2vec_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(w2vec_X_train, y_train)\n",
    "predicted = model.predict(w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e93e04",
   "metadata": {},
   "source": [
    "### Bonus: let's use pre-trained vectors instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe81189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model again\n",
    "pre_trained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# re-vectorize the Xs - make sure to specify the right size for the embeddings\n",
    "pre_trained_w2vec_X_train = np.array([sentence_to_embedding(_, pre_trained_model, stop_words, dims=50) for _ in X_train])\n",
    "pre_trained_w2vec_X_test = np.array([sentence_to_embedding(_, pre_trained_model, stop_words, dims=50) for _ in X_test])\n",
    "print(len(pre_trained_w2vec_X_train))\n",
    "pre_trained_w2vec_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(pre_trained_w2vec_X_train, y_train)\n",
    "predicted = model.predict(pre_trained_w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4390229",
   "metadata": {},
   "source": [
    "## Word2vec for analogies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e47c4aa",
   "metadata": {},
   "source": [
    "A famous property of word2vec is the ability to capture analogical relations through the embedding space, such as for example:\n",
    "\n",
    "man : king = woman : ?\n",
    "\n",
    "We re-use a pre-trained model, trained on wikipedia, to show how analogies are encoded in the latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a0b34b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get a model trained on twitter: https://radimrehurek.com/gensim/auto_examples/howtos/run_downloader_api.html\n",
    "pre_trained_model = api.load(\"glove-twitter-50\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021db603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def analogy(model, worda, wordb, wordc):\n",
    "    result = model.most_similar(negative=[worda], \n",
    "                                positive=[wordb, wordc])\n",
    "    return result[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0ab02d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(analogy(pre_trained_model, 'king', 'man', 'queen'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6372a907",
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = ['australia', 'canada', 'germany', 'ireland', 'italy']\n",
    "foods = [analogy(pre_trained_model, 'us', 'hamburger', country) for country in countries]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5818e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for c, f in zip(countries, foods):\n",
    "    print(c, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118994ed",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d8aa",
   "metadata": {},
   "source": [
    "We have discussed how to turn word into vectors using neural network - can we do the same to the entire sentence, without recurring to the mean trick?\n",
    "\n",
    "YES, but training models that work well on sentences require a huge amount of computation. However, the same logic applies here: we can take a model that has been pre-trained on a very large corpus, and use it to vectorize our finance dataset.\n",
    "\n",
    "As an example, we will use the convenient sentence transformer (https://github.com/UKPLab/sentence-transformers) to map text to a dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a242de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684aa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run example code to check all is good with the library\n",
    "sentences = [\n",
    "    'This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.'\n",
    "]\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding[:10])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sentence_encoding(sentences, model):\n",
    "    # this takes a while!\n",
    "    embedded_sentences = model.encode(sentences)\n",
    "    assert len(embedded_sentences) == len(sentences)\n",
    "    \n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01739239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-vectorize the Xs - make sure to specify the right size for the embeddings\n",
    "bert_w2vec_X_train = np.array(bert_sentence_encoding(X_train, sentence_model))\n",
    "bert_w2vec_X_test = np.array(bert_sentence_encoding(X_test, sentence_model))\n",
    "print(bert_w2vec_X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(bert_w2vec_X_train, y_train)\n",
    "predicted = model.predict(bert_w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0085fc57",
   "metadata": {},
   "source": [
    "_Let's use a small NN for classification as well!_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f56c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import some scienc-y stuff\n",
    "from keras.layers.core import Dropout\n",
    "from keras.layers.core import Dense\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import SGD\n",
    "from keras.models import Model\n",
    "from keras import utils\n",
    "from keras.callbacks import EarlyStopping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caa800c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_dataset_for_classification(Xs: list, Ys: list, category_2_id: dict):\n",
    "    _X = np.array(bert_sentence_encoding(Xs, sentence_model))\n",
    "    _Y = list()\n",
    "    for y in Ys:\n",
    "        target_as_int = category_2_id[y]\n",
    "        # keras needs the target to be a one-hot vector for the target category\n",
    "        _Y.append(utils.to_categorical(target_as_int, num_classes=len(category_2_id)))\n",
    "\n",
    "    return _X, np.array(_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f834a7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map categories to id and viceversa - in this dataset obv trivial but the example code\n",
    "# will work in more complex classification projects\n",
    "c_2_id = {0: 0, 1:1, 2:2}\n",
    "id_2_c = {0: 0, 1:1, 2:2}\n",
    "k_x_train, k_y_train = prepare_dataset_for_classification(X_train, y_train, c_2_id)\n",
    "k_x_test, k_y_test = prepare_dataset_for_classification(X_test, y_test, c_2_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19220e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(k_x_train[0][:10], k_y_train[0])\n",
    "print(len(k_x_train), len(k_y_train), len(k_x_test), len(k_y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b66fb9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(input_d, target_classes):\n",
    "    print('Shape tensor {}, target classes {}'.format(input_d, target_classes))\n",
    "    # define a model\n",
    "    model = Sequential()\n",
    "    model.add(Dense(64, activation='relu', input_dim=input_d))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(target_classes, activation='softmax'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3636fdc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_keras_model(x_train, y_train, x_test, y_test, epochs=2000, batch_size=32, patience=25):\n",
    "    keras_model = build_model(x_train[0].shape[0], y_train[0].shape[0])\n",
    "    sgd = SGD(lr=0.01)\n",
    "    es = EarlyStopping(monitor='loss', mode='min', verbose=1, patience=patience, restore_best_weights=True)\n",
    "    keras_model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])\n",
    "    keras_model.fit(x_train, y_train, epochs=epochs, batch_size=batch_size, callbacks=[es])\n",
    "    score = keras_model.evaluate(x_test, y_test, batch_size=batch_size)\n",
    "    print(\"Evaluation metrics: {}\".format(score))\n",
    "\n",
    "    return keras_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "976cc505",
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_classifier = train_and_evaluate_keras_model(k_x_train, k_y_train, k_x_test, k_y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55a0eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_confusion_matrix_and_report(target_model, x_test, y_test_golden): \n",
    "    Y_pred = target_model.predict(x_test)\n",
    "    y_pred = np.argmax(Y_pred, axis=1)\n",
    "    cm = confusion_matrix(y_test_golden, y_pred)\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_test_golden, y_pred))\n",
    "    plot_confusion_matrix(cm)\n",
    "                                          \n",
    "    return\n",
    "\n",
    "y_test_golden = np.argmax(k_y_test, axis=1)\n",
    "calculate_confusion_matrix_and_report(neural_classifier, k_x_test, y_test_golden)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e76b8a4",
   "metadata": {},
   "source": [
    "What is the conceptual difference between having a fully neural pipeline and a \"mixed\" one? TL;DR: a fully neural pipeline can do end-to-end learning, by using one uniform update strategy during training, i.e. back-propagation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f264b3c8",
   "metadata": {},
   "source": [
    "### Bonus point: the softmax function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e69b832",
   "metadata": {},
   "source": [
    "If you read the keras model carefully you will find a softmax function at the end of our neural network:\n",
    "\n",
    "(target_classes, activation='softmax')\n",
    "\n",
    "The last layer of a NN designed for classification is typically the size of the target space (i.e. how many possible categories I have?), and softmax is the function that takes as input the vector of values from the one-before-last layer, and generate a probability distribution over the target classes, i.e. a distribution whose values sum up to 1. \n",
    "\n",
    "In classification we usually take the highest value, but the softmax is useful as it can be applied to any number of classes and somehow encodes how uncertain the model is about the prediction: NOTE THAT USING SOFTMAX OUTPUT AS A RIGOROUS DEFINITION OF UNCERTAINTY IS NOT CORRECT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf583bb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def softmax(xs):\n",
    "    return np.exp(xs) / sum(np.exp(xs))\n",
    "\n",
    "print(softmax(np.array([-1, 0, 3, 5])))\n",
    "print(softmax(np.array([0.09, -99.9, 0.88, 23])))\n",
    "print(sum(softmax(np.array([-1, 0, 3, 5]))))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
