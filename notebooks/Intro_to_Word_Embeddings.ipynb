{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "underlying-payroll",
   "metadata": {},
   "source": [
    "# Intro to Word Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "934cd420",
   "metadata": {},
   "source": [
    "In this notebook we learn about word2vec as a new technique to convert words into (dense) vectors, and re-build the text classification pipeline we had previously built with sparse vectors.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, so some functions are re-used from previous lectures to provide a self-sufficient script)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diagnostic-buying",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some global imports\n",
    "import json\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identical-disability",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462b028b",
   "metadata": {},
   "source": [
    "## Bonus: perceptron as the simplest NN"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be6ce2e6",
   "metadata": {},
   "source": [
    "Taking the code (with some minor tweaks) from the fantastic Perceptron class here (https://www.thomascountz.com/2018/04/05/19-line-line-by-line-python-perceptron), this is a simple and transparent implementation of a Perceptron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ef8d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Perceptron(object):\n",
    "\n",
    "    def __init__(self, no_of_inputs):\n",
    "        # initialize the w + bias array\n",
    "        self.weights = np.zeros(no_of_inputs + 1)\n",
    "        \n",
    "        return\n",
    "           \n",
    "    def predict(self, inputs):\n",
    "        summation = np.dot(inputs, self.weights[1:]) + self.weights[0]\n",
    "        \n",
    "        return 1 if summation > 0 else 0\n",
    "\n",
    "    def train(self, training_inputs, labels, epochs=100, learning_rate=0.01):\n",
    "        for _ in range(epochs):\n",
    "            for inputs, label in zip(training_inputs, labels):\n",
    "                prediction = self.predict(inputs)\n",
    "                # update the weights\n",
    "                self.weights[1:] += learning_rate * (label - prediction) * inputs\n",
    "                # update the bias\n",
    "                self.weights[0] += learning_rate * (label - prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1dd47ca",
   "metadata": {},
   "source": [
    "To understand how the forward pass works, let's do some quick calculations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d5bae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [1, 1]\n",
    "weights = [0, 0]\n",
    "bias = 1 \n",
    "inputs_dot_weights = np.dot(np.array(inputs), np.array(weights))\n",
    "_sum = inputs_dot_weights + bias\n",
    "\n",
    "print(inputs_dot_weights, _sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f577e6",
   "metadata": {},
   "source": [
    "We can use the class to learn some real-world function of interest, for example the AND function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91ce165f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's learn the AND operator\n",
    "training_inputs = []\n",
    "training_inputs.append(np.array([1, 1]))\n",
    "training_inputs.append(np.array([1, 0]))\n",
    "training_inputs.append(np.array([0, 1]))\n",
    "training_inputs.append(np.array([0, 0]))\n",
    "# 1 only when both inputs are 1\n",
    "labels = np.array([1, 0, 0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e42aa79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate class and train\n",
    "perceptron = Perceptron(2)\n",
    "# weights before training\n",
    "print(perceptron.weights)\n",
    "perceptron.train(training_inputs, labels)\n",
    "# weights after training\n",
    "print(perceptron.weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5252f66",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(perceptron.predict(np.array([1, 1])))\n",
    "print(perceptron.predict(np.array([0, 1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ab0544",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4889c4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff40c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [[pre_process_sentence(_['sentence']), _['label']] for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3699cbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_dataset = get_finance_sentences()\n",
    "# print out the first items in the dataset, to check the format\n",
    "finance_dataset[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c11e2a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get sentences without label for vectorizer part\n",
    "finance_dataset_sentences = [_[0] for _ in finance_dataset]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-season",
   "metadata": {},
   "source": [
    "## From words to vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "internal-seller",
   "metadata": {},
   "source": [
    "As you may recall, we introduced some \"vectorizing\" procedures for text before, e.g. TfidfVectorizer. As you may recall, these vectors are very long and sparse - we quikcly re-create some here for convenience:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-channel",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "docs = finance_dataset_sentences[:2]\n",
    "tfidfvectorizer = TfidfVectorizer(analyzer='word')\n",
    "tfidf_wm = tfidfvectorizer.fit_transform(docs)\n",
    "tfidf_tokens = tfidfvectorizer.get_feature_names()\n",
    "df_tfidfvect = pd.DataFrame(data=tfidf_wm.toarray(),\n",
    "                            index=['Doc{}'.format(_) for _ in range(len(docs))], \n",
    "                            columns=tfidf_tokens)\n",
    "print(\"TD-IDF Vectorizer\\n\")\n",
    "print(df_tfidfvect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f79d602a",
   "metadata": {},
   "source": [
    "Let us know use word2vec to get vectors for words first, and document after. We will use a fantastic Python library, gensim: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e458e0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install gensim==4.0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93c56c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7113d0f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_word2vec_model(\n",
    "    sentences: list,\n",
    "    min_count: int = 2,\n",
    "    vector_size: int = 48,\n",
    "    window: int = 2,\n",
    "    epochs: int = 20\n",
    "):\n",
    "    \"\"\"\n",
    "    Sentences is a list of lists, where each list is composed by tokens in a sentence: e.g.\n",
    "    \n",
    "    [\n",
    "        ['the', 'cat', 'is', 'on' ...],\n",
    "        ['i', 'live', 'in', 'nyc', ...],\n",
    "        ....\n",
    "    ]\n",
    "    \n",
    "    \"\"\"\n",
    "    model =  gensim.models.Word2Vec(sentences=sentences,\n",
    "                                    min_count=min_count,\n",
    "                                    vector_size=vector_size,\n",
    "                                    window=window,\n",
    "                                    epochs=epochs)\n",
    "    \n",
    "    # this is how many words we will have in the space\n",
    "    print(\"# words in the space: {}\".format(len(model.wv.index_to_key)))\n",
    "\n",
    "    # we return the space in a format that will allow us to do nice things afterwards ;-)    \n",
    "    return model.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1fd0596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's use nltk tokenizer to break up sentences and build a word2vec model\n",
    "# https://www.nltk.org/api/nltk.tokenize.html\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "print(finance_dataset_sentences[0], '\\n\\n', word_tokenize(finance_dataset_sentences[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6aab115",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_sentences = [word_tokenize(_) for _ in finance_dataset_sentences]\n",
    "# debug \n",
    "tokenized_sentences[:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76bb4aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build a counter to get a sense of the lexicon\n",
    "word_counter = Counter([item for sent in tokenized_sentences for item in sent])\n",
    "word_counter.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "611ca10c",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model = train_word2vec_model(tokenized_sentences)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeca8664",
   "metadata": {},
   "source": [
    "Now that we have a vector space, let's find words similar to a given term..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b77f5c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), w2v_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce47d345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: what happens as the window grows bigger? What is your prediction?\n",
    "\n",
    "# w2v_model = train_word2vec_model(tokenized_sentences, window=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1467f3a2",
   "metadata": {},
   "source": [
    "To get a sense of what the vectors look like, we print them out in 2D using TSNE (https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60a5e17b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_scatter_by_category_with_lookup(title, \n",
    "                                         words, \n",
    "                                         word_to_target_cat,\n",
    "                                         results):\n",
    "    \"\"\"\n",
    "    Just a plotting routine\n",
    "    \"\"\"\n",
    "    \n",
    "    groups = {}\n",
    "    for word, target_cat in word_to_target_cat.items():\n",
    "        if word not in words:\n",
    "            continue\n",
    "\n",
    "        word_idx = words.index(word)\n",
    "        x = results[word_idx][0]\n",
    "        y = results[word_idx][1]\n",
    "        if target_cat in groups:\n",
    "            groups[target_cat]['x'].append(x)\n",
    "            groups[target_cat]['y'].append(y)\n",
    "        else:\n",
    "            groups[target_cat] = {\n",
    "                'x': [x], 'y': [y]\n",
    "                }\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    for group, data in groups.items():\n",
    "        ax.scatter(data['x'], data['y'], \n",
    "                   alpha=0.1 if group == 0 else 0.8, \n",
    "                   edgecolors='none', \n",
    "                   s=25, \n",
    "                   marker='o',\n",
    "                   label=group)\n",
    "\n",
    "    plt.title(title)\n",
    "    plt.legend(loc=2)\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d3365d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tsne_analysis(embeddings, perplexity=25, n_iter=500):\n",
    "    tsne = TSNE(n_components=2, verbose=1, perplexity=perplexity, n_iter=n_iter)\n",
    "    return tsne.fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c14dce69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# map words to known categories of interest\n",
    "\n",
    "# 0 is the generic category\n",
    "words = w2v_model.index_to_key\n",
    "print(len(words))\n",
    "words_to_category = {w: 0 for w in words}\n",
    "# manually pick some words to display\n",
    "for w in ['company', 'profit', 'investment', 'loss', 'margin', 'group']:\n",
    "    words_to_category[w] = 1\n",
    "for w in ['with', 'of', 'from', 'by', 'as']:\n",
    "    words_to_category[w] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa3852",
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = [w2v_model[w] for w in words]\n",
    "tsne_results = tsne_analysis(embeddings)\n",
    "assert len(tsne_results) == len(words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7e1e78",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Finance word2vec', words, words_to_category, tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b393ff0",
   "metadata": {},
   "source": [
    "_Why the quality is not ideal?_\n",
    "\n",
    "Our dataset is very small, and word2vec works much better when large corpora are used. However, a pretty cool things of language is that is everywhere: the word \"company\" is very important in the financial sector, but of course also Wikipedia talks a lot about companies... can we make use of all the text out there?\n",
    "\n",
    "The answer is YES: in particular, a pattern that is common to many NLP (but also vision-related) tasks is to initialize a model with PRE-TRAINED embeddings, obtained previously with training on large corpora. We could either re-use them or \"fine-tune\" them: in either case, we will, so to speak, be able to harness the power of Wikipedia even in a corpus very small such as ours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d488aa",
   "metadata": {},
   "source": [
    "### Bonus: using pre-trained embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "working-strengthening",
   "metadata": {},
   "source": [
    "Here we use Gensim-data to recover dense vectors for words in our vocabulary, as pre-trained on Wikipedia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1df9fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5c0c979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove-wiki-gigaword-50 (400000 records): Pre-trained vectors based on Wikipedia\n",
    "pre_trained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# test it out\n",
    "for w in ['company', 'profit']:\n",
    "    print('\\n======>{}\\n'.format(w), pre_trained_model.similar_by_word(w, topn=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a5410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "words =[w for w in w2v_model.index_to_key if w in pre_trained_model]\n",
    "print(len(words))\n",
    "pre_trained_vectors = [pre_trained_model[w] for w in words]\n",
    "pre_trained_tsne_results = tsne_analysis(pre_trained_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6554dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_scatter_by_category_with_lookup('Finance pre-trained word2vec', \n",
    "                                     words, \n",
    "                                     words_to_category, \n",
    "                                     pre_trained_tsne_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9364273",
   "metadata": {},
   "source": [
    "## Application: Text Classification Revisited"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9efe9b6",
   "metadata": {},
   "source": [
    "As you may recall, one text is in a vectorized form, the downstream pipeline we learned through scikit can be applied in the same way to language dataset. For convenience, we report again a standard classifier for financial news built with TF-IDF transformation first, and then use word2vec to the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34ea7e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "finance_dataset_text = [_[0] for _ in finance_dataset]\n",
    "finance_dataset_label = [_[1] for _ in finance_dataset]\n",
    "all_labels = set(finance_dataset_label)\n",
    "print(\"All labels are: {}\".format(all_labels))\n",
    "X_train, X_test, y_train, y_test = train_test_split(finance_dataset_text, \n",
    "                                                    finance_dataset_label, \n",
    "                                                    test_size=0.1, \n",
    "                                                    random_state=42)\n",
    "\n",
    "print(len(X_train))\n",
    "final_tfidfvectorizer = TfidfVectorizer(analyzer='word', stop_words='english')\n",
    "final_tfidf_train = final_tfidfvectorizer.fit_transform(X_train)\n",
    "print(final_tfidf_train.shape)\n",
    "X_test_transformed = final_tfidfvectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645b418d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(final_tfidf_train, y_train)\n",
    "predicted = model.predict(X_test_transformed)\n",
    "predicted_prob = model.predict_proba(X_test_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a1b402",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "def calculate_confusion_matrix_and_report(y_predicted, y_golden, with_plot=True):\n",
    "    # calculate confusion matrix: \n",
    "    cm = confusion_matrix(y_golden, y_predicted)\n",
    "    # build a readable report;\n",
    "    # https://scikit-learn.org/stable/modules/model_evaluation.html#classification-report\n",
    "    print('\\nClassification Report')\n",
    "    print(classification_report(y_golden, y_predicted))\n",
    "    # plot the matrix\n",
    "    if with_plot:\n",
    "        plot_confusion_matrix(cm)\n",
    "                                          \n",
    "    return\n",
    "                                          \n",
    "def plot_confusion_matrix(c_matrix):\n",
    "    plt.imshow(c_matrix, cmap=plt.cm.Blues)\n",
    "    plt.xlabel(\"Predicted labels\")\n",
    "    plt.ylabel(\"True labels\")\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ad36ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e9740d",
   "metadata": {},
   "source": [
    "Let us know transform sentences using word2vec - we go through each of the sentence, remove stop words and take the average of the vector if present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ada8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug some vars to make sure all is in order\n",
    "print(w2v_model.most_similar(\"company\"))\n",
    "print(X_train[0],y_train[0])\n",
    "print(X_test[0], y_test[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f283f1c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = stopwords.words('english')\n",
    "stop_words[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "040f24c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence, stop_words):\n",
    "    return [w for w in word_tokenize(sentence) if w not in stop_words]\n",
    "\n",
    "\n",
    "def sentence_to_embedding(sentence, model, stop_words, dims=48):\n",
    "    tokenized_sentence = tokenize(sentence, stop_words)\n",
    "    if not tokenized_sentence:\n",
    "        print(\"\\n!!!ATTENTION!!! Empty sentence: {}\".format(sentence))\n",
    "        return np.zeros(dims)\n",
    "    mean_array = np.mean([model[w] for w in tokenized_sentence if w in model] or [np.zeros(dims)], axis=0)\n",
    "    assert len(mean_array) == dims\n",
    "    \n",
    "    return np.array(mean_array)\n",
    "\n",
    "# debug\n",
    "_test = 'company profits were soaring last year'\n",
    "print(tokenize(_test, stop_words))\n",
    "print(sentence_to_embedding(_test, w2v_model, stop_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eec9fc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q: instead of taking the average, can we weight \"more\" embeddings which are more important?\n",
    "# e.g. can we use tf-idf as a weighting scheme to aggregate word vectors?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a21c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "w2vec_X_train = np.array([sentence_to_embedding(_, w2v_model, stop_words) for _ in X_train])\n",
    "w2vec_X_test = np.array([sentence_to_embedding(_, w2v_model, stop_words) for _ in X_test])\n",
    "print(len(w2vec_X_train))\n",
    "w2vec_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1e09b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(w2vec_X_train, y_train)\n",
    "predicted = model.predict(w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0e93e04",
   "metadata": {},
   "source": [
    "### Bonus: let's use pre-trained vectors instead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe81189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the model again\n",
    "pre_trained_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# re-vectorize the Xs - make sure to specify the right size for the embeddings\n",
    "pre_trained_w2vec_X_train = np.array([sentence_to_embedding(_, pre_trained_model, stop_words, dims=50) for _ in X_train])\n",
    "pre_trained_w2vec_X_test = np.array([sentence_to_embedding(_, pre_trained_model, stop_words, dims=50) for _ in X_test])\n",
    "print(len(pre_trained_w2vec_X_train))\n",
    "pre_trained_w2vec_X_train[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e171ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(pre_trained_w2vec_X_train, y_train)\n",
    "predicted = model.predict(pre_trained_w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118994ed",
   "metadata": {},
   "source": [
    "## What's next?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e71d8aa",
   "metadata": {},
   "source": [
    "We have discussed how to turn word into vectors using neural network - can we do the same to the entire sentence, without recurring to the mean trick?\n",
    "\n",
    "YES, but training models that work well on sentences require a huge amount of computation. However, the same logic applies here: we can take a model that has been pre-trained on a very large corpus, and use it to vectorize our finance dataset.\n",
    "\n",
    "As an example, we will use the convenient sentence transformer (https://github.com/UKPLab/sentence-transformers) to map text to a dense vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a242de",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "sentence_model = SentenceTransformer('stsb-distilbert-base')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684aa46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# run example code to check all is good with the library\n",
    "sentences = [\n",
    "    'This framework generates embeddings for each input sentence',\n",
    "    'Sentences are passed as a list of string.', \n",
    "    'The quick brown fox jumps over the lazy dog.'\n",
    "]\n",
    "sentence_embeddings = sentence_model.encode(sentences)\n",
    "for sentence, embedding in zip(sentences, sentence_embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding[:10])\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95169c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_sentence_encoding(sentences, model):\n",
    "    # this takes a while!\n",
    "    embedded_sentences = model.encode(sentences)\n",
    "    assert len(embedded_sentences) == len(sentences)\n",
    "    \n",
    "    return embedded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01739239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# re-vectorize the Xs - make sure to specify the right size for the embeddings\n",
    "bert_w2vec_X_train = np.array(bert_sentence_encoding(X_train, sentence_model))\n",
    "bert_w2vec_X_test = np.array(bert_sentence_encoding(X_test, sentence_model))\n",
    "print(bert_w2vec_X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8479ea67",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestClassifier(n_estimators=200, max_depth=3, random_state=0)\n",
    "model.fit(bert_w2vec_X_train, y_train)\n",
    "predicted = model.predict(bert_w2vec_X_test)\n",
    "print(\"Total of # {} test cases\".format(len(y_test)))\n",
    "calculate_confusion_matrix_and_report(predicted, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c876a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69a6f70c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
