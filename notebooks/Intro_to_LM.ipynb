{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "055e0daa",
   "metadata": {},
   "source": [
    "# Intro to Language Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd606380",
   "metadata": {},
   "source": [
    "In this notebook we experiment with good ol' Markovian Language Models. \n",
    "\n",
    "While most contemporary use of LMs is neural, discussing old LMs will help us introduce some terminology, build some intuition on what (doesn't) work(s) and why and, more generally, start getting our hands dirty with language data within a very \"interpretable\" setting.\n",
    "\n",
    "You are encouraged to play around with the code and modify / re-built parts of it as you fit: there is NO substitute for \"tinkering with code\" to understand how all the concepts fit together (corollary: all this code is written for pedagogical purposes, not for production use)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e3cdda",
   "metadata": {},
   "source": [
    "## Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c537bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make sure you have the datasets library installed\n",
    "# see: https://github.com/huggingface/datasets\n",
    "\n",
    "# !pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f70083f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "# some utils function\n",
    "def get_finance_sentiment_dataset(split: str='sentences_allagree'):\n",
    "    # load financial dataset from HF\n",
    "    from datasets import load_dataset\n",
    "    # https://huggingface.co/datasets/financial_phrasebank\n",
    "    # by default, load just sentences for which all annotators agree\n",
    "    dataset = load_dataset(\"financial_phrasebank\", split)\n",
    "    \n",
    "    return dataset['train']\n",
    "\n",
    "\n",
    "def get_finance_sentences():\n",
    "    dataset = get_finance_sentiment_dataset()\n",
    "    cleaned_dataset = [prepare_sentence(_['sentence']) for _ in dataset]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from finance dataset\\n\".format(len(cleaned_dataset)))\n",
    "    \n",
    "    return cleaned_dataset\n",
    "\n",
    "\n",
    "def prepare_sentence(sentence: str):\n",
    "    processed_sentence = pre_process_sentence(sentence)\n",
    "    \n",
    "    return tokenize_sentence(processed_sentence)\n",
    "\n",
    "\n",
    "def pre_process_sentence(sentence: str):\n",
    "    # this choices are VERY important. Here, we take a simplified \n",
    "    # view, remove the punctuations and just lower case everything\n",
    "    lower_sentence = sentence.lower()\n",
    "    # remove punctuation\n",
    "    # nice suggestion from https://stackoverflow.com/questions/265960/best-way-to-strip-punctuation-from-a-string\n",
    "    # if we change the exclude set, we can control what to exclude\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in lower_sentence if ch not in exclude)\n",
    "\n",
    "\n",
    "def tokenize_sentence(sentence: str):\n",
    "    # we use a vanilla tokenization technique (tokenize on white spaces): \n",
    "    # in production you may want to use a specialized\n",
    "    # library to achieve the same goal, for example, the snippet below from https://spacy.io/api/tokenizer\n",
    "    # shows how to tokenize an English sentence with Spacy\n",
    "    \n",
    "    # from spacy.lang.en import English\n",
    "    # nlp = English()\n",
    "    # Create a Tokenizer with the default settings for English\n",
    "    # including punctuation rules and exceptions\n",
    "    # tokenizer = nlp.tokenizer\n",
    "    # tokens = tokenizer(\"This is a sentence\")\n",
    "    return sentence.split()\n",
    "\n",
    "\n",
    "def get_corpus_from_text_file(text_file: str):\n",
    "    # from a text file, we return a list of lists, where each list is a token in a sentence\n",
    "    # ATTENTION: to get sentences we just split on new lines, remove empty lines and then split\n",
    "    # on punctuation (;, .)\n",
    "    # In a real setting, you would use specific libraries to detect sentence boundaries.\n",
    "    with open(text_file, 'r') as file:\n",
    "        sentences = [_ for _ in [s.strip() for s in file.read().replace(';', '.').split('.')] if _]\n",
    "        # debug \n",
    "        print(\"{} raw sentences found in {}\".format(len(sentences), text_file))\n",
    "    \n",
    "    # clean the sentences and remove empty ones\n",
    "    cleaned_sentences = [_ for _ in [prepare_sentence(s) for s in sentences] if _]\n",
    "    # debug \n",
    "    print(\"{} cleaned sentences from {}\\n\".format(len(cleaned_sentences), text_file))\n",
    "            \n",
    "    return cleaned_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bad363cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some test cases - get in the habit of test your functions ;-)\n",
    "assert len(tokenize_sentence(\"This is my test sentence\")) == 5\n",
    "assert prepare_sentence(\"This is my sentence\") == [\"this\", \"is\", \"my\", \"sentence\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aecba71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we load different corpora here, so that we can switch between them later on\n",
    "DATASETS = {}\n",
    "# some shakespeare stuff, https://www.gutenberg.org/ebooks/author/65\n",
    "DATASETS['william'] = get_corpus_from_text_file('shakespeare.txt')\n",
    "# some Paul Graham stuff, http://www.paulgraham.com/articles.html\n",
    "DATASETS['paul'] = get_corpus_from_text_file('graham.txt')\n",
    "# some Finance stuff\n",
    "DATASETS['finance'] = get_finance_sentences()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084525ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "389793ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# debug by printing out some random sentences\n",
    "for _ in range(5):\n",
    "    print(choice(DATASETS['finance']), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b24a0aa2",
   "metadata": {},
   "source": [
    "## Zipf law in action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9b504",
   "metadata": {},
   "source": [
    "A recurrent problem in NLP is that a lot of what is said is relatively rare. According to Zipf Law (https://en.wikipedia.org/wiki/Zipf%27s_law), the most frequent word will occur approximately twice as often as the second most frequent word.\n",
    "\n",
    "Let's do some counts and plot in our small datasets to get a feeling of what it means in practice..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bf0368",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f01a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_words_from_dataset(dataset: list):\n",
    "    # we first FLAT a list of list \n",
    "    # [['i', 'am', 'jacopo'], ['you', 'are', 'funny']] ->\n",
    "    # ['i', 'am', 'jacopo', 'you', 'are', 'funny']\n",
    "    # and feed the list to a counter object\n",
    "    return [word for sentence in dataset for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4861a92b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = [['i', 'am', 'jacopo'], ['you', 'are', 'funny']]\n",
    "assert get_all_words_from_dataset(test_set) == ['i', 'am', 'jacopo', 'you', 'are', 'funny']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8f3967f",
   "metadata": {},
   "outputs": [],
   "source": [
    "counters = {}\n",
    "for d, dataset in DATASETS.items():\n",
    "    # get all words in a corpus\n",
    "    all_words = get_all_words_from_dataset(dataset)\n",
    "    counters[d] = Counter(all_words)\n",
    "    # print out the most common words!\n",
    "    print(d, counters[d].most_common(10), '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f236c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus - we use the awesome package here: https://arxiv.org/pdf/1305.0215.pdf\n",
    "# to do a proper powerlaw fit (paper is cool, if you want to see what is behind this)\n",
    "\n",
    "# FROM https://github.com/jeffalstott/powerlaw\n",
    "\n",
    "#!pip install powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a14eaf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import powerlaw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7493a331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we fit word frequency data\n",
    "all_words = get_all_words_from_dataset(DATASETS['finance'])\n",
    "c = Counter(all_words)\n",
    "data = [_[1] for _ in c.most_common()]\n",
    "print(data[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61881d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit = powerlaw.Fit(data, discrete=True)\n",
    "# compare the powerlaw fit and plot it\n",
    "fit.distribution_compare('power_law', 'lognormal')\n",
    "fig = fit.plot_ccdf(linewidth=3, label='Empirical Data')\n",
    "fit.power_law.plot_ccdf(ax=fig, color='r', linestyle='--', label='Power law fit')\n",
    "fit.lognormal.plot_ccdf(ax=fig, color='g', linestyle='--', label='Lognormal fit')\n",
    "fig.set_ylabel(u\"p(Xâ‰¥x)\")\n",
    "fig.set_xlabel(\"Word Frequency\")\n",
    "handles, labels = fig.get_legend_handles_labels()\n",
    "fig.legend(handles, labels, loc=3)\n",
    "figname = 'FigLognormal'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "941c448e",
   "metadata": {},
   "source": [
    "## Vanilla Language Modelling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "374c4d08",
   "metadata": {},
   "source": [
    "Thanks to the Markov assumption, we can use empirical frequencies to learn a language model for a given corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56c1f766",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting n-grams\n",
    "def find_ngrams(tokens: list, n: int=2):\n",
    "    # this is pretty cool: http://www.locallyoptimal.com/blog/2013/01/20/elegant-n-gram-generation-in-python/\n",
    "    # try to understand what is going on here ;-)\n",
    "    return zip(*[tokens[i:] for i in range(n)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0678703",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(find_ngrams(['this', 'is', 'NYC', 'and', 'I', 'love', 'it'], n=1)))\n",
    "print(list(find_ngrams(['this', 'is', 'NYC', 'and', 'I', 'love', 'it'], n=2)))\n",
    "print(list(find_ngrams(['this', 'is', 'NYC', 'and', 'I', 'love', 'it'], n=3)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7433886b",
   "metadata": {},
   "outputs": [],
   "source": [
    "assert len(list(find_ngrams(['this', 'is', 'NYC'], n=2))) == 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38c0e24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# nltk is another cool NLP package, which comes with some ngram and LM functionalities out of the box\n",
    "\n",
    "#!pip install nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbc38846",
   "metadata": {},
   "source": [
    "While it won't always be possible for us to do both a \"from scracth\" and a \"package\" implementation, Markov LM are a good test case (as in, the methods are straightforward and transparent enough to be re-created in a notebook)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ed23f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.util import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4420fac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(ngrams(['this', 'is', 'NYC', 'and', 'I', 'love', 'it'], n=3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0833026d",
   "metadata": {},
   "source": [
    "We introduce some custom tokens to indicate the start/end of a sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3deb7641",
   "metadata": {},
   "outputs": [],
   "source": [
    "START_SYMBOL = 'FRE_7773_START'\n",
    "STOP_SYMBOL = 'FRE_7773_STOP'\n",
    "\n",
    "def pad_tokens(tokens: list, n=2):\n",
    "    return [START_SYMBOL] * (n - 1) + tokens + [STOP_SYMBOL]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7c7462",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pad_tokens(['I', 'love', 'it'], n=3))\n",
    "assert pad_tokens(['this', 'is', 'NYC']) == [START_SYMBOL, 'this', 'is', 'NYC', STOP_SYMBOL]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1027ca72",
   "metadata": {},
   "source": [
    "### Training a vanilla language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b0a97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ngram_counter_for_lm(corpus: list, k: int):\n",
    "    all_ngrams = []\n",
    "    # loop over sentences, pad them with START and STOP symbol, and generate all ngram up until the chosen k\n",
    "    for sentence in corpus:\n",
    "        cnt_sentence = pad_tokens(sentence, n=k)\n",
    "        for _ in range(1, k + 1): \n",
    "            all_ngrams = all_ngrams + list(find_ngrams(cnt_sentence, n=_))\n",
    "\n",
    "    ngram_counter = Counter(all_ngrams)\n",
    "    # debug\n",
    "    print(ngram_counter.most_common(10))\n",
    "    \n",
    "    return ngram_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "575dbe74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do a test run with a small corpus\n",
    "cnt_corpus = [\n",
    "    ['i', 'love', 'nyc'],\n",
    "    ['i', 'teach', 'ml', 'in', 'nyc'],\n",
    "    ['mike', 'lives', 'in', 'nyc'],\n",
    "    ['mike', 'loves', 'nyc'],\n",
    "    ['john', 'loves', 'chicago'],\n",
    "    ['mike', 'is', 'a', 'great', 'teacher'],\n",
    "    ['nyc', 'is', 'a', 'great', 'city'],\n",
    "    ['chicago', 'is', 'a', 'big', 'city'],\n",
    "    ['chicago', 'is', 'a', 'clean', 'city'],\n",
    "    ['teaching', 'ml', 'is', 'great'],\n",
    "    ['my', 'favorite', 'city', 'is', 'nyc']\n",
    "]\n",
    "n = 2\n",
    "bigram_lm = get_ngram_counter_for_lm(corpus=cnt_corpus, k=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad70994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import log\n",
    "\n",
    "def calculate_sentence_probability(sentence: str, ngram_lm: Counter, n: int=2, verbose=False):\n",
    "    tokens = prepare_sentence(sentence)\n",
    "    cnt_sentence = pad_tokens(tokens, n=n)\n",
    "    n_grams = list(find_ngrams(cnt_sentence, n=n))\n",
    "    if verbose:\n",
    "        print(cnt_sentence)\n",
    "        print(n_grams)\n",
    "    prob = 0.0\n",
    "    for n_gram in n_grams:\n",
    "        n_gram_count = ngram_lm[n_gram]\n",
    "        n_minus_1_count = ngram_lm[n_gram[:-1]]\n",
    "        n_gram_probability = n_gram_count / n_minus_1_count\n",
    "        # debug\n",
    "        if verbose:\n",
    "            print(n_gram, n_gram[:-1], n_gram_count, n_minus_1_count)\n",
    "        prob = prob + log(n_gram_probability)\n",
    "    \n",
    "    return prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b34bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sentences = [\n",
    "    'mike loves chicago',\n",
    "    'nyc is a big city',\n",
    "    'nyc is a clean city'\n",
    "]\n",
    "for s in test_sentences:\n",
    "    print(s, calculate_sentence_probability(s, bigram_lm, n), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b956d3",
   "metadata": {},
   "source": [
    "_What happens when a test sentence features unseen ngrams?_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620539f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_sentence_probability('teaching ml in chicago is great', bigram_lm, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3ee7b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def build_probability_map(ngram_lm: Counter, n: int=2):\n",
    "    # build a map storing the probability that a given n-gram follows a n-1 -gram\n",
    "    n_gram_map = defaultdict(list)\n",
    "    for (n_gram, count) in ngram_lm.most_common():\n",
    "        # debug\n",
    "        # print(n_gram, len(n_gram), count)\n",
    "        if len(n_gram) == n:\n",
    "            n_gram_map[n_gram[:-1]].append((n_gram, count))\n",
    "    \n",
    "    return n_gram_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0791d600",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_gram_map = build_probability_map(bigram_lm, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b285508b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the bigram starting with the START SYMBOL\n",
    "n_gram_map[(START_SYMBOL,)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5dec01",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence(prompt: str, n_gram_map: dict, n: int=2, is_random=False):\n",
    "    tokens = prepare_sentence(prompt)\n",
    "    # remove the STOP symbol at the right\n",
    "    sentence = pad_tokens(tokens, n=n)[:-1]\n",
    "    while STOP_SYMBOL not in sentence or len(sentence) == 20:\n",
    "        n_gram_key = tuple(sentence[len(sentence) - (n - 1):])\n",
    "        # debug\n",
    "        # print(sentence, n_gram_key)\n",
    "        # possible continuations\n",
    "        continuations = n_gram_map[n_gram_key]\n",
    "        # pick the first one, or a random one\n",
    "        new_token = continuations[0][0][n - 1:] if not is_random else choice(continuations)[0][n - 1:]\n",
    "        sentence = sentence + list(new_token)\n",
    "                              \n",
    "    return ' '.join(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6a1eb82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sentence\n",
    "generate_sentence('mike', n_gram_map, n, is_random=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3f372e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sentence\n",
    "generate_sentence('mike', n_gram_map, n, is_random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2b77fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_sentence('', n_gram_map, n, is_random=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdd9c657",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now run it on one of our dataset\n",
    "cnt_corpus = DATASETS['william']\n",
    "n = 2 \n",
    "bigram_lm = get_ngram_counter_for_lm(corpus=cnt_corpus, k=n)\n",
    "n_gram_map = build_probability_map(bigram_lm, n=n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5059a362",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print all the bigram starting with the START SYMBOL\n",
    "n_gram_map[(START_SYMBOL,)][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75329224",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sentence\n",
    "generate_sentence('what', n_gram_map, n)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fadc7e",
   "metadata": {},
   "source": [
    "_A trigram model has the same underlying logic..._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e666cee9",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnt_corpus = DATASETS['finance']\n",
    "n = 3\n",
    "trigram_lm = get_ngram_counter_for_lm(corpus=cnt_corpus, k=n)\n",
    "n_gram_map = build_probability_map(trigram_lm, n=n)\n",
    "n_gram_map[(START_SYMBOL, 'the')][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32cf7fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate a sentence\n",
    "generate_sentence('the new', n_gram_map, n, is_random=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "135da930",
   "metadata": {},
   "source": [
    "## Training a LM with NLTK"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93481e94",
   "metadata": {},
   "source": [
    "Now that we understand how a simple n-gram model works, we can use some abstraction to train and test it faster (the following code uses NLTK API -> https://www.nltk.org/api/nltk.lm.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f341d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm.preprocessing import padded_everygram_pipeline\n",
    "cnt_corpus = DATASETS['finance']\n",
    "n = 3\n",
    "train_data, padded_sents = padded_everygram_pipeline(3, cnt_corpus)\n",
    "# what does the new train_data vaiable hold? It is simply a collection of n-grams up to n for the sentences\n",
    "# in the corpus\n",
    "print(list(list(train_data)[0])[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44d5b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.lm import MLE, Laplace\n",
    "\n",
    "def create_nltk_model(corpus: list, n: int):\n",
    "    train_data, padded_sents = padded_everygram_pipeline(n, corpus)\n",
    "    ngram_model = Laplace(n)\n",
    "    ngram_model.fit(train_data, padded_sents)\n",
    "    # debug - print the vocabulary\n",
    "    print(len(ngram_model.vocab))\n",
    "    \n",
    "    return ngram_model\n",
    "\n",
    "# we now fit the model to the data\n",
    "cnt_corpus = DATASETS['paul']\n",
    "n = 3\n",
    "_model = create_nltk_model(cnt_corpus, n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba0c5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_nltk(model, num_words):\n",
    "    content = []\n",
    "    for token in model.generate(num_words):\n",
    "        if token == '<s>':\n",
    "            continue\n",
    "        if token == '</s>':\n",
    "            break\n",
    "        content.append(token)\n",
    "        \n",
    "    return ' '.join(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488ebd52",
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(5):\n",
    "    print(generate_sentence_nltk(_model, 20), '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f2d62a",
   "metadata": {},
   "source": [
    "## Evaluate a Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e13e61c",
   "metadata": {},
   "source": [
    "NTLK comes with an out of the box method to calculate perplexity - we can see it in action here on a sample training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048c8536",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [['a', 'b'], ['a', 'b', 'c'], ['a', 'c', 'd', 'c', 'e', 'f']]\n",
    "train, vocab = padded_everygram_pipeline(2, text)\n",
    "lm = MLE(2)\n",
    "lm.fit(train, vocab)\n",
    "# print(lm.score(\"b\", [\"a\"]))\n",
    "# print(lm.logscore(\"a\"))\n",
    "# print(lm.logscore(\"b\", [\"a\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5af9613",
   "metadata": {},
   "outputs": [],
   "source": [
    "from statistics import mean\n",
    "\n",
    "def calculate_perplexity(log_probs: list):\n",
    "    return 2** (-1 * mean(log_probs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9627ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test = [('a', 'b'), ('c', 'd')]\n",
    "log_probs = [lm.logscore(t[-1], t[:-1]) for t in test]\n",
    "\n",
    "# calculate perplexity from scratch and with the built-in function, as a double check!\n",
    "print(lm.perplexity(test))\n",
    "print(calculate_perplexity(log_probs))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc73244a",
   "metadata": {},
   "source": [
    "We now turn a more \"realistic\" LM, splitting one of our dataset in train and test set and measuring perplexity..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b98077bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_corpus, test_corpus = train_test_split(DATASETS['paul'], test_size=0.1)\n",
    "print(len(train_corpus), len(test_corpus))\n",
    "test_corpus[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a49edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 2\n",
    "_model = create_nltk_model(train_corpus, n)\n",
    "_model.perplexity(test_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3482b316",
   "metadata": {},
   "source": [
    "Based on our domain knowledge, we could also evaluate a model through some \"behavioral checks\" (e.g. https://arxiv.org/abs/2005.04118), that is, we can prepare input-output pairs and make sure the model behave as expected in edge cases...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6965e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def paul_behavioral_checks(nltk_model):\n",
    "    # write some behavioral tests based on paul graham known themes...\n",
    "    assert nltk_model.logscore(\"startup\") > nltk_model.logscore(\"politics\")\n",
    "    assert nltk_model.counts[\"nerds\"] > 0\n",
    "    assert nltk_model.logscore(\"founder\", [\"startup\"]) > nltk_model.logscore(\"founder\", [\"italian\"])\n",
    "    \n",
    "    print(\"All checks passed!\")\n",
    "    return \n",
    "\n",
    "\n",
    "paul_behavioral_checks(_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cf5d28",
   "metadata": {},
   "source": [
    "## Applied LM: How to Write a Spelling Corrector (Norvig's Post)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44203ec5",
   "metadata": {},
   "source": [
    "Language models are important components of many NLP application. A pedagogically extraordinary post by Peter Norvig (https://norvig.com/spell-correct.html) shows how to implement a noisy channel model in Python for typo correction.\n",
    "\n",
    "The code below is taken directly from the original post (with some minor modifications to work with our corpora) and it is reported here for convenience: one of your homework assignment asks you to improve upon his original work by either improving the language model or the error model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c18a60f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# pick a corpus\n",
    "cnt_corpus = DATASETS['paul']\n",
    "# get word frequencies\n",
    "WORDS = Counter(get_all_words_from_dataset(cnt_corpus))\n",
    "WORDS.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85e25f6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "    \"Probability of `word`.\"\n",
    "    return WORDS[word] / N\n",
    "\n",
    "def correction(word): \n",
    "    \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059709fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def unit_tests():\n",
    "    assert correction('beause') == 'because'                # insert\n",
    "    assert correction('srartupz') == 'startup'              # replace 2\n",
    "    assert correction('bycycle') == 'bicycle'               # replace\n",
    "    assert correction('inconvient') == 'inconvenient'       # insert 2\n",
    "    assert correction('arrainged') == 'arranged'            # delete\n",
    "    assert correction('peotry') =='poetry'                  # transpose\n",
    "    assert correction('peotryy') =='poetry'                 # transpose + delete\n",
    "    assert correction('word') == 'word'                     # known\n",
    "    assert correction('quintessential') == 'quintessential' # unknown\n",
    "    assert WORDS.most_common(1)[0][0] == 'the'\n",
    "    assert P('trafalgar') == 0\n",
    "\n",
    "    return 'unit_tests pass'\n",
    "\n",
    "\n",
    "def spelltest(tests, verbose=False):\n",
    "    \"Run correction(wrong) on all (right, wrong) pairs; report results.\"\n",
    "    import time\n",
    "    start = time.time()\n",
    "    good, unknown = 0, 0\n",
    "    n = len(tests)\n",
    "    for right, wrong in tests:\n",
    "        w = correction(wrong)\n",
    "        good += (w == right)\n",
    "        if w != right:\n",
    "            unknown += (right not in WORDS)\n",
    "            if verbose:\n",
    "                print('correction({}) => {} ({}); expected {} ({})'.format(wrong, w, WORDS[w], right, WORDS[right]))\n",
    "    dt = time.time() - start\n",
    "    print('{:.0%} of {} correct ({:.0%} unknown) at {:.0f} words per second '\n",
    "          .format(good / n, n, unknown / n, n / dt))\n",
    "    \n",
    "    return\n",
    "\n",
    "\n",
    "def Testset(lines):\n",
    "    \"Parse 'right: wrong1 wrong2' lines into [('right', 'wrong1'), ('right', 'wrong2')] pairs.\"\n",
    "    return [(right, wrong)\n",
    "            for (right, wrongs) in (line.split(':') for line in lines)\n",
    "            for wrong in wrongs.split()]\n",
    "\n",
    "print(unit_tests())\n",
    "spelltest(Testset(open('spell-testset1.txt'))) # Development set\n",
    "spelltest(Testset(open('spell-testset2.txt'))) # Final test set"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
